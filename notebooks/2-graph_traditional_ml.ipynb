{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning with Graph Features\n",
    "## Notebook 2\n",
    "\n",
    "In this notebook, we will train to ML models to peform selling price predictions. The first will use traditional, one-hot encoded features, while the second will incorporated graph-derived features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connect to TigerGraph Database\n",
    "\n",
    "The code block below connects to a TigerGraph database. Make sure to change the authentication details in order for you to connect to the instance successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyTigerGraph import TigerGraphConnection\n",
    "\n",
    "conn=TigerGraphConnection(\n",
    "    host=\"YOUR_HOSTNAME_HERE\",\n",
    "    graphname=\"KDD_2022_NFT\",\n",
    "    gsqlSecret=\"YOUR_SECRET_HERE\"\n",
    ")\n",
    "conn.getToken(\"YOUR_SECRET_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activate Machine Learning Workbench Data Loaders\n",
    "In order to use Kafka for data loading, we need to activate the functionality on the database. To do this, replace `YOUR_HOSTNAME_HERE` and `YOUR_SECRET_HERE` with your credentials below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlwb activate YOUR_HOST_HERE -s YOUR_SECRET_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "conn.gds.configureKafka(kafka_address=\"kaf.kdd.tigergraphlabs.com:19092\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split the Data\n",
    "\n",
    "We are going to use the built-in data splitter to split the data between training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = conn.gds.vertexSplitter(v_types=[\"Transaction\"], train=0.8, test=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Traditional Feature Approach\n",
    "\n",
    "Here, we are going to train a neural network to perform regression of the selling price of the NFT, given the category and collection one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = conn.gds.vertexLoader(\n",
    "    attributes={\"Transaction\": [\"usd_price\", \"categoryOneHot\", \"collectionOneHot\"]},\n",
    "    filter_by=\"train\",\n",
    "    batch_size=2048,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "nn = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1075, 1000),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1000, 1000),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1000, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 1)\n",
    ")\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(nn.parameters(), lr=0.01)\n",
    "loss = torch.nn.SmoothL1Loss()\n",
    "mae = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def clean_onehots(x, length):\n",
    "    arr = np.fromstring(x, sep=\" \", dtype=np.float32)\n",
    "    if len(arr) > length:\n",
    "        arr = arr[:length]\n",
    "    elif len(arr) < length:\n",
    "        arr = np.zeros(length)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "\n",
    "writer = SummaryWriter('runs/no_graph_feats_training'+str(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    epoch_loss = 0\n",
    "    epoch_mae = 0\n",
    "    j = 0\n",
    "    for batch in train_loader:\n",
    "        catOH = torch.tensor(np.stack(batch[\"Transaction\"][\"categoryOneHot\"].apply(lambda x: clean_onehots(x, 6)).values).astype(np.float32))\n",
    "        collOH = torch.tensor(np.stack(batch[\"Transaction\"][\"collectionOneHot\"].apply(lambda x: clean_onehots(x, 1069)).values).astype(np.float32))\n",
    "        X = torch.tensor(np.concatenate([catOH, collOH], axis=1))\n",
    "        y = torch.tensor(batch[\"Transaction\"][\"usd_price\"].values.astype(np.float32))\n",
    "        out = nn(X).flatten()\n",
    "        loss_val = loss(out, y)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        mae_val = mae(out, y).item()\n",
    "        epoch_loss += loss_val.item()\n",
    "        epoch_mae += mae_val\n",
    "\n",
    "        \n",
    "        writer.add_scalar('training loss',\n",
    "                        loss_val.item(),\n",
    "                        i * train_loader.num_batches + j)\n",
    "        writer.add_scalar('training mae',\n",
    "                          mae_val,\n",
    "                          i * train_loader.num_batches + j)\n",
    "\n",
    "        j += 1\n",
    "    print(\"Epoch:\", i, \"Loss:\", epoch_loss/train_loader.num_batches, \"MAE:\", epoch_mae/train_loader.num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup TensorBoard\n",
    "\n",
    "Once we setup our TensorBoard Writer in the cell above, we can create the TensorBoard visualization in TigerGraph ML Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "notebook_pvc = '-'.join(os.environ['HOSTNAME'].split('-')[:-1] + ['volume'])\n",
    "\n",
    "log_dir = \"kdd2022-tutorial/notebooks/runs\"\n",
    "\n",
    "def kubectl_cmd(yaml, action, *arg):\n",
    "    return f\"cat <<EOF | kubectl {action} -f -\" + yaml.format(*arg) + \"EOF\"\n",
    "\n",
    "\n",
    "yaml_tb = \"\"\"\n",
    "apiVersion: tensorboard.kubeflow.org/v1alpha1\n",
    "kind: Tensorboard\n",
    "metadata:\n",
    "  name: my-tensorboard\n",
    "spec:\n",
    "  logspath: pvc://{0}/{1}\n",
    "\"\"\"\n",
    "\n",
    "cmd = kubectl_cmd(yaml_tb, 'apply', notebook_pvc, log_dir)\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we run this command, you can navigate back to the ML Workbench homepage. Click on **Tensorboards** on the left hand menu bar. This should take you to a screen like this:\n",
    "\n",
    "<img src=\"../img/tensorBoardTab.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "On the row that corresponds to **my-tensorboard**, click **Connect**. This will bring you to your Tensorboard page where you can monitor the training progress of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = conn.gds.vertexLoader(\n",
    "    attributes={\"Transaction\": [\"usd_price\", \"categoryOneHot\", \"collectionOneHot\"]},\n",
    "    filter_by=\"test\",\n",
    "    batch_size=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_sum = 0\n",
    "for batch in test_loader:\n",
    "    catOH = torch.tensor(np.stack(batch[\"Transaction\"][\"categoryOneHot\"].apply(lambda x: clean_onehots(x, 6)).values).astype(np.float32))\n",
    "    collOH = torch.tensor(np.stack(batch[\"Transaction\"][\"collectionOneHot\"].apply(lambda x: clean_onehots(x, 1069)).values).astype(np.float32))\n",
    "    X = torch.tensor(np.concatenate([catOH, collOH], axis=1))\n",
    "    y = torch.tensor(batch[\"Transaction\"][\"usd_price\"].values.astype(np.float32))\n",
    "    with torch.no_grad():\n",
    "        out = nn(X).flatten()\n",
    "        mae_sum += mae(out, y).item()\n",
    "print(\"MAE:\", mae_sum/test_loader.num_batches,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add Graph Features\n",
    "\n",
    "**HANDS ON CODE:** Check `query_answers` directory if you are not participating in the live tutorial.\n",
    "\n",
    "We want to enrich the model with graph-based features. Lets create some features that incorporate community and centrality information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./seller_pagerank.gsql\n",
    "\n",
    "\n",
    "CREATE QUERY seller_pagerank(BOOL print_accum = FALSE, STRING result_attr = \"\") {\n",
    "    transactions = {Transaction.*};\n",
    "    SumAccum<DOUBLE> @seller_pr;\n",
    "    MaxAccum<DOUBLE> @@max_seller_pr;\n",
    "\n",
    "\n",
    "    res = SELECT t FROM transactions:t -(NFT_SOLD_BY)-> NFT_User:u \n",
    "          ACCUM\n",
    "            t.@seller_pr += u.pagerank,\n",
    "            @@max_seller_pr += u.pagerank\n",
    "          POST-ACCUM\n",
    "            IF result_attr != \"\" THEN\n",
    "                t.setAttr(result_attr, t.@seller_pr/@@max_seller_pr)\n",
    "            END;\n",
    "    IF print_accum THEN\n",
    "      PRINT res[res.@seller_pr];\n",
    "    END;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = conn.gds.featurizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.installAlgorithm(\"seller_pagerank\", query_path=\"./seller_pagerank.gsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"result_attr\": \"seller_pr\"}\n",
    "\n",
    "featurizer.runAlgorithm(\"seller_pagerank\", params, feat_name=\"seller_pr\", feat_type=\"DOUBLE\", custom_query=True, schema_name=[\"Transaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./buyer_pagerank.gsql\n",
    "\n",
    "\n",
    "CREATE QUERY buyer_pagerank(BOOL print_accum = FALSE, STRING result_attr = \"\") {\n",
    "    transactions = {Transaction.*};\n",
    "    SumAccum<DOUBLE> @buyer_pr;\n",
    "    MaxAccum<DOUBLE> @@max_buyer_pr;\n",
    "\n",
    "\n",
    "    res = SELECT t FROM transactions:t -(NFT_BOUGHT_BY)-> NFT_User:u \n",
    "          ACCUM\n",
    "            t.@buyer_pr += u.pagerank,\n",
    "            @@max_buyer_pr += u.pagerank\n",
    "          POST-ACCUM\n",
    "            IF result_attr != \"\" THEN\n",
    "                t.setAttr(result_attr, t.@buyer_pr/@@max_buyer_pr)\n",
    "            END;\n",
    "    IF print_accum THEN\n",
    "      PRINT res[res.@buyer_pr];\n",
    "    END;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.installAlgorithm(\"buyer_pagerank\", query_path=\"./buyer_pagerank.gsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"result_attr\": \"buyer_pr\"}\n",
    "\n",
    "featurizer.runAlgorithm(\"buyer_pagerank\", params, feat_name=\"buyer_pr\", feat_type=\"DOUBLE\", custom_query=True, schema_name=[\"Transaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./kcore_size.gsql\n",
    "\n",
    "\n",
    "CREATE QUERY kcore_size(BOOL print_accum = FALSE, STRING result_attr = \"\") FOR GRAPH KDD_2022_NFT { \n",
    "  MapAccum<INT, SumAccum<FLOAT>> @@kcore_size;\n",
    "  MaxAccum<FLOAT> @@max_kcore_size;\n",
    "\n",
    "  \n",
    "  nftuser = {NFT_User.*};\n",
    "  \n",
    "  res = SELECT t FROM nftuser:t POST-ACCUM @@kcore_size += (t.k_core -> 1);\n",
    "  \n",
    "  IF print_accum THEN\n",
    "    PRINT @@kcore_size;\n",
    "  END;\n",
    "\n",
    "  FOREACH (key, value) IN @@kcore_size DO\n",
    "    @@max_kcore_size += value;\n",
    "  END;\n",
    "  \n",
    "  IF result_attr != \"\" THEN\n",
    "    res = SELECT t FROM nftuser:t POST-ACCUM t.setAttr(result_attr, @@kcore_size.get(t.k_core)/@@max_kcore_size);\n",
    "  END;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.installAlgorithm(\"kcore_size\", query_path=\"./kcore_size.gsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"result_attr\": \"kcore_size\"}\n",
    "\n",
    "featurizer.runAlgorithm(\"kcore_size\", params, feat_name=\"kcore_size\", feat_type=\"DOUBLE\", custom_query=True, schema_name=[\"NFT_User\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./seller_kcore_size.gsql\n",
    "\n",
    "\n",
    "CREATE QUERY seller_kcore_size(BOOL print_accum = FALSE, STRING result_attr = \"\") {\n",
    "    transactions = {Transaction.*};\n",
    "    SumAccum<DOUBLE> @seller_k_size;\n",
    "\n",
    "\n",
    "    res = SELECT t FROM transactions:t -(NFT_SOLD_BY)-> NFT_User:u \n",
    "          ACCUM\n",
    "            t.@seller_k_size += u.kcore_size\n",
    "          POST-ACCUM\n",
    "            IF result_attr != \"\" THEN\n",
    "                t.setAttr(result_attr, t.@seller_k_size)\n",
    "            END;\n",
    "    IF print_accum THEN\n",
    "      PRINT res[res.@seller_k_size];\n",
    "    END;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.installAlgorithm(\"seller_kcore_size\", query_path=\"./seller_kcore_size.gsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"result_attr\": \"seller_k_size\"}\n",
    "\n",
    "featurizer.runAlgorithm(\"seller_kcore_size\", params, feat_name=\"seller_k_size\", feat_type=\"DOUBLE\", custom_query=True, schema_name=[\"Transaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./buyer_kcore_size.gsql\n",
    "\n",
    "\n",
    "CREATE QUERY buyer_kcore_size(BOOL print_accum = FALSE, STRING result_attr = \"\") {\n",
    "    transactions = {Transaction.*};\n",
    "    SumAccum<DOUBLE> @buyer_k_size;\n",
    "\n",
    "\n",
    "    res = SELECT t FROM transactions:t -(NFT_BOUGHT_BY)-> NFT_User:u \n",
    "          ACCUM\n",
    "            t.@buyer_k_size += u.kcore_size\n",
    "          POST-ACCUM\n",
    "            IF result_attr != \"\" THEN\n",
    "                t.setAttr(result_attr, t.@buyer_k_size)\n",
    "            END;\n",
    "    IF print_accum THEN\n",
    "      PRINT res[res.@buyer_k_size];\n",
    "    END;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.installAlgorithm(\"buyer_kcore_size\", query_path=\"./buyer_kcore_size.gsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"result_attr\": \"buyer_k_size\"}\n",
    "\n",
    "featurizer.runAlgorithm(\"buyer_kcore_size\", params, feat_name=\"buyer_k_size\", feat_type=\"DOUBLE\", custom_query=True, schema_name=[\"Transaction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Neural Network with Graph-Based Features\n",
    "\n",
    "Using the same size (apart from the input dimension) of neural network, lets train and evaluate a model using both traditional and graph-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = conn.gds.vertexLoader(\n",
    "    attributes={\"Transaction\": [\"buyer_k_size\", \"seller_k_size\", \"usd_price\", \"seller_pr\", \"buyer_pr\", \"categoryOneHot\", \"collectionOneHot\"]},\n",
    "    filter_by=\"train\",\n",
    "    batch_size=2048,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "nn = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1079, 1000),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1000, 1000),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1000, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 1)\n",
    ")\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(nn.parameters(), lr=0.01)\n",
    "loss = torch.nn.SmoothL1Loss()\n",
    "mae = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/graph_feats_training'+str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    epoch_loss = 0\n",
    "    epoch_mae = 0\n",
    "    j = 0\n",
    "    for batch in train_loader:\n",
    "        catOH = torch.tensor(np.stack(batch[\"Transaction\"][\"categoryOneHot\"].apply(lambda x: clean_onehots(x, 6)).values).astype(np.float32))\n",
    "        collOH = torch.tensor(np.stack(batch[\"Transaction\"][\"collectionOneHot\"].apply(lambda x: clean_onehots(x, 1069)).values).astype(np.float32))\n",
    "        X = torch.tensor(np.concatenate([batch[\"Transaction\"][[\"seller_k_size\", \"buyer_k_size\", \"seller_pr\", \"buyer_pr\"]].values.astype(np.float32), catOH, collOH], axis=1))\n",
    "        y = torch.tensor(batch[\"Transaction\"][\"usd_price\"].values.astype(np.float32))\n",
    "        out = nn(X).flatten()\n",
    "        loss_val = loss(out, y)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        mae_val = mae(out, y).item()\n",
    "        epoch_loss += loss_val.item()\n",
    "        epoch_mae += mae_val\n",
    "        writer.add_scalar('training loss',\n",
    "                        loss_val.item(),\n",
    "                        i * train_loader.num_batches + j)\n",
    "        writer.add_scalar('training mae',\n",
    "                          mae_val,\n",
    "                          i * train_loader.num_batches + j)\n",
    "        j += 1\n",
    "    print(\"Epoch:\", i, \"Loss:\", epoch_loss/train_loader.num_batches, \"MAE:\", epoch_mae/train_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = conn.gds.vertexLoader(\n",
    "    attributes={\"Transaction\": [\"seller_k_size\", \"buyer_k_size\", \"usd_price\", \"seller_pr\", \"buyer_pr\", \"categoryOneHot\", \"collectionOneHot\"]},\n",
    "    filter_by=\"test\",\n",
    "    batch_size=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_sum = 0\n",
    "for batch in test_loader:\n",
    "    catOH = torch.tensor(np.stack(batch[\"Transaction\"][\"categoryOneHot\"].apply(lambda x: clean_onehots(x, 6)).values).astype(np.float32))\n",
    "    collOH = torch.tensor(np.stack(batch[\"Transaction\"][\"collectionOneHot\"].apply(lambda x: clean_onehots(x, 1069)).values).astype(np.float32))\n",
    "    X = torch.tensor(np.concatenate([batch[\"Transaction\"][[\"seller_k_size\", \"buyer_k_size\", \"seller_pr\", \"buyer_pr\"]].values.astype(np.float32), catOH, collOH], axis=1))\n",
    "    y = torch.tensor(batch[\"Transaction\"][\"usd_price\"].values.astype(np.float32))\n",
    "    with torch.no_grad():\n",
    "        out = nn(X).flatten()\n",
    "        mae_sum += mae(out, y).item()\n",
    "print(\"MAE:\", mae_sum/test_loader.num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Determine Graph Feature Importance\n",
    "\n",
    "Using **Captum**, we can determine the attribution scores of each graph-based feature to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import GradientShap\n",
    "\n",
    "gs = GradientShap(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes a little while to run\n",
    "\n",
    "train_X = []\n",
    "\n",
    "\n",
    "for train_batch in train_loader:\n",
    "    catOH = torch.tensor(np.stack(train_batch[\"Transaction\"][\"categoryOneHot\"].apply(lambda x: clean_onehots(x, 6)).values).astype(np.float32))\n",
    "    collOH = torch.tensor(np.stack(train_batch[\"Transaction\"][\"collectionOneHot\"].apply(lambda x: clean_onehots(x, 1069)).values).astype(np.float32))\n",
    "    train_x = torch.tensor(np.concatenate([train_batch[\"Transaction\"][[\"seller_k_size\", \"buyer_k_size\", \"seller_pr\", \"buyer_pr\"]].values.astype(np.float32), catOH, collOH], axis=1))\n",
    "    train_X.append(train_x)\n",
    "train_X = torch.concat(train_X)\n",
    "\n",
    "test_X = []\n",
    "for test_batch in test_loader:\n",
    "    catOH = torch.tensor(np.stack(test_batch[\"Transaction\"][\"categoryOneHot\"].apply(lambda x: clean_onehots(x, 6)).values).astype(np.float32))\n",
    "    collOH = torch.tensor(np.stack(test_batch[\"Transaction\"][\"collectionOneHot\"].apply(lambda x: clean_onehots(x, 1069)).values).astype(np.float32))\n",
    "    test_x = torch.tensor(np.concatenate([test_batch[\"Transaction\"][[\"seller_k_size\", \"buyer_k_size\", \"seller_pr\", \"buyer_pr\"]].values.astype(np.float32), catOH, collOH], axis=1))\n",
    "    test_X.append(test_x)\n",
    "test_X = torch.concat(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution = gs.attribute(test_X, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_nt_attr_test_sum = attribution.detach().numpy().sum(0)\n",
    "ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(ig_nt_attr_test_sum, ord=1)\n",
    "\n",
    "attributions = pd.DataFrame({\"Feature\": [\"Seller_k_size\", \"Buyer_k_size\", \"Seller_pr\", \"Buyer_pr\"], \"Attribution\": ig_nt_attr_test_norm_sum[:4]})\n",
    "\n",
    "plt = attributions.plot(kind=\"bar\", xlabel=\"Feature\", ylabel=\"Attribution\", title=\"Attribution of Graph Features to NFT Price\")\n",
    "plt.set_xticklabels(attributions.Feature, rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TigerGraph Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "58e1c476a485e41cfbe41589baaac1849e255b67efca409760b0ec34d7f4c191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
