{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyTigerGraph as tg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "conn = tg.TigerGraphConnection(\"http://3.144.132.94\", graphname=\"KDD_2022_NFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = conn.gds.vertexSplitter(v_types=[\"Transaction\"], train=0.8, test=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting vertices...\n",
      "Vertex split finished successfully.\n"
     ]
    }
   ],
   "source": [
    "splitter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./seller_pagerank.gsql\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./seller_pagerank.gsql\n",
    "\n",
    "CREATE QUERY seller_pagerank(BOOL print_accum = FALSE, STRING result_attr = \"\") {\n",
    "    transactions = {Transaction.*};\n",
    "    SumAccum<DOUBLE> @seller_pr;\n",
    "\n",
    "\n",
    "    res = SELECT t FROM transactions:t -(NFT_SOLD_BY)-> NFT_User:u \n",
    "          ACCUM\n",
    "            t.@seller_pr += u.pagerank\n",
    "          POST-ACCUM\n",
    "            IF result_attr != \"\" THEN\n",
    "                t.setAttr(result_attr, t.@seller_pr)\n",
    "            END;\n",
    "    IF print_accum THEN\n",
    "      PRINT res[res.@seller_pr];\n",
    "    END;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = conn.gds.featurizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seller_pagerank'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.installAlgorithm(\"seller_pagerank\", query_path=\"./seller_pagerank.gsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"result_attr\": \"seller_pr\"}\n",
    "\n",
    "try:\n",
    "    featurizer.runAlgorithm(\"seller_pagerank\", params, feat_name=\"seller_pr\", feat_type=\"DOUBLE\", custom_query=True, schema_name=[\"Transaction\"])\n",
    "except ConnectionError:\n",
    "    featurizer.runAlgorithm(\"seller_pagerank\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./buyer_pagerank.gsql\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./buyer_pagerank.gsql\n",
    "\n",
    "CREATE QUERY buyer_pagerank(BOOL print_accum = FALSE, STRING result_attr = \"\") {\n",
    "    transactions = {Transaction.*};\n",
    "    SumAccum<DOUBLE> @buyer_pr;\n",
    "\n",
    "\n",
    "    res = SELECT t FROM transactions:t -(NFT_BOUGHT_BY)-> NFT_User:u \n",
    "          ACCUM\n",
    "            t.@buyer_pr += u.pagerank\n",
    "          POST-ACCUM\n",
    "            IF result_attr != \"\" THEN\n",
    "                t.setAttr(result_attr, t.@buyer_pr)\n",
    "            END;\n",
    "    IF print_accum THEN\n",
    "      PRINT res[res.@buyer_pr];\n",
    "    END;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buyer_pagerank'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.installAlgorithm(\"buyer_pagerank\", query_path=\"./buyer_pagerank.gsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"result_attr\": \"buyer_pr\"}\n",
    "\n",
    "try:\n",
    "    featurizer.runAlgorithm(\"buyer_pagerank\", params, feat_name=\"buyer_pr\", feat_type=\"DOUBLE\", custom_query=True, schema_name=[\"Transaction\"])\n",
    "except ConnectionError:\n",
    "    featurizer.runAlgorithm(\"buyer_pagerank\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./kcore_size.gsql\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./kcore_size.gsql\n",
    "\n",
    "CREATE QUERY kcore_size(BOOL print_accum = FALSE, STRING result_attr = \"\") FOR GRAPH KDD_2022_NFT { \n",
    "  MapAccum<INT, SumAccum<INT>> @@kcore_size;\n",
    "  \n",
    "  trans = {Transaction.*};\n",
    "  \n",
    "  res = SELECT t FROM trans:t POST-ACCUM @@kcore_size += (t.k_core -> 1);\n",
    "  \n",
    "  IF print_accum THEN\n",
    "    PRINT @@kcore_size;\n",
    "  END;\n",
    "  \n",
    "  IF result_attr != \"\" THEN\n",
    "    res = SELECT t FROM trans:t POST-ACCUM t.setAttr(result_attr, @@kcore_size.get(t.k_core));\n",
    "  END;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and optimizing the queries, it might take a minute\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kcore_size'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.installAlgorithm(\"kcore_size\", query_path=\"./kcore_size.gsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"result_attr\": \"kcore_size\"}\n",
    "\n",
    "try:\n",
    "    featurizer.runAlgorithm(\"kcore_size\", params, feat_name=\"kcore_size\", feat_type=\"INT\", custom_query=True, schema_name=[\"Transaction\"])\n",
    "except ConnectionError:\n",
    "    featurizer.runAlgorithm(\"kcore_size\", params, custom_query=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = conn.getSchema(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and optimizing queries. It might take a minute if this is the first time you use this loader.\n",
      "Query installation finished.\n"
     ]
    }
   ],
   "source": [
    "train_loader = conn.gds.vertexLoader(\n",
    "    attributes={\"Transaction\": [\"kcore_size\", \"usd_price\", \"seller_pr\", \"buyer_pr\"]},\n",
    "    filter_by=\"train\",\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "nn = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1000),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1000, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 1)\n",
    ")\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(nn.parameters(), lr=0.01)\n",
    "loss = torch.nn.SmoothL1Loss()\n",
    "mae = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 348.6691375125772 MAE: 349.10870423535454 R2: -418.2236720564552\n",
      "Loss: 143.34268744743738 MAE: 143.80907596732087 R2: -0.04300043910661476\n",
      "Loss: 143.3426495986486 MAE: 143.80939821412943 R2: -0.042974666122477655\n",
      "Loss: 143.34267037641007 MAE: 143.80941925408706 R2: -0.04297420952840635\n",
      "Loss: 143.34269116702427 MAE: 143.80943305023277 R2: -0.04297374554400174\n",
      "Loss: 143.3427050248632 MAE: 143.8094446794042 R2: -0.04297329858949563\n",
      "Loss: 143.34271578775903 MAE: 143.80945503615305 R2: -0.04297293614184439\n",
      "Loss: 143.34272531422002 MAE: 143.8094629354554 R2: -0.04297266462742479\n",
      "Loss: 143.34273096943159 MAE: 143.80946885543372 R2: -0.04297245127171519\n",
      "Loss: 143.34273438312295 MAE: 143.8094738731487 R2: -0.042972284185918516\n",
      "Loss: 143.34273967074577 MAE: 143.80947605554627 R2: -0.042972177829382556\n",
      "Loss: 143.3427403879294 MAE: 143.8094779423305 R2: -0.04297208753878858\n",
      "Loss: 143.34274287879308 MAE: 143.80948037407148 R2: -0.042972031307991626\n",
      "Loss: 143.34274584520858 MAE: 143.80948133545743 R2: -0.0429720101009482\n",
      "Loss: 143.34274517429486 MAE: 143.80948075451298 R2: -0.04297197475587582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/parkererickson/kdd2022-tutorial/notebooks/graph_traditional_ml.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/kdd2022-tutorial/notebooks/graph_traditional_ml.ipynb#ch0000014?line=2'>3</a>\u001b[0m epoch_mae \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/kdd2022-tutorial/notebooks/graph_traditional_ml.ipynb#ch0000014?line=3'>4</a>\u001b[0m epoch_r2 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/parkererickson/kdd2022-tutorial/notebooks/graph_traditional_ml.ipynb#ch0000014?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/kdd2022-tutorial/notebooks/graph_traditional_ml.ipynb#ch0000014?line=5'>6</a>\u001b[0m     X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(batch[\u001b[39m\"\u001b[39m\u001b[39mTransaction\u001b[39m\u001b[39m\"\u001b[39m][[\u001b[39m\"\u001b[39m\u001b[39mkcore_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mseller_pr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbuyer_pr\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/kdd2022-tutorial/notebooks/graph_traditional_ml.ipynb#ch0000014?line=6'>7</a>\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(batch[\u001b[39m\"\u001b[39m\u001b[39mTransaction\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39musd_price\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kdd2022/lib/python3.9/site-packages/pyTigerGraph/gds/dataloaders.py:910\u001b[0m, in \u001b[0;36mBaseLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m--> 910\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_q\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m    911\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kdd2022/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kdd2022/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    epoch_loss = 0\n",
    "    epoch_mae = 0\n",
    "    epoch_r2 = 0\n",
    "    for batch in train_loader:\n",
    "        X = torch.tensor(batch[\"Transaction\"][[\"kcore_size\", \"seller_pr\", \"buyer_pr\"]].values.astype(np.float32))\n",
    "        y = torch.tensor(batch[\"Transaction\"][\"usd_price\"].values.astype(np.float32))\n",
    "        out = nn(X).flatten()\n",
    "        loss_val = loss(out, y)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss_val.item()\n",
    "        epoch_mae += mae(out, y).item()\n",
    "        epoch_r2 += r2_loss(out, y).item()\n",
    "    print(\"Loss:\", epoch_loss/train_loader.num_batches, \"MAE:\", epoch_mae/train_loader.num_batches, \"R2:\", epoch_r2/train_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = conn.gds.vertexLoader(\n",
    "    attributes={\"Transaction\": [\"kcore_size\", \"usd_price\", \"seller_pr\", \"buyer_pr\"]},\n",
    "    filter_by=\"test\",\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 92.19558295797795 R2: -0.0536285676854722\n"
     ]
    }
   ],
   "source": [
    "mae_sum = 0\n",
    "r2_sum = 0\n",
    "for batch in test_loader:\n",
    "    X = torch.tensor(batch[\"Transaction\"][[\"kcore_size\", \"seller_pr\", \"buyer_pr\"]].values.astype(np.float32))\n",
    "    y = torch.tensor(batch[\"Transaction\"][\"usd_price\"].values.astype(np.float32))\n",
    "    with torch.no_grad():\n",
    "        out = nn(X).flatten()\n",
    "        mae_sum += mae(out, y).item()\n",
    "        r2_sum += r2_loss(out, y).item()\n",
    "print(\"MAE:\", mae_sum/test_loader.num_batches, \"R2:\", r2_sum/test_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretLoader = conn.gds.vertexLoader(\n",
    "    attributes={\"Transaction\": [\"kcore_size\", \"usd_price\", \"seller_pr\", \"buyer_pr\"]},\n",
    "    filter_by=\"test\",\n",
    "    num_batches=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(interpretLoader.data[\"Transaction\"][[\"kcore_size\", \"seller_pr\", \"buyer_pr\"]].values.astype(np.float32))\n",
    "y = torch.tensor(interpretLoader.data[\"Transaction\"][\"usd_price\"].values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Model with Captum\n",
    "https://captum.ai/tutorials/House_Prices_Regression_Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from captum library\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(nn)\n",
    "ig_nt = NoiseTunnel(ig)\n",
    "dl = DeepLift(nn)\n",
    "#gs = GradientShap(nn)\n",
    "fa = FeatureAblation(nn)\n",
    "\n",
    "ig_attr_test = ig.attribute(X, n_steps=50)\n",
    "ig_nt_attr_test = ig_nt.attribute(X)\n",
    "dl_attr_test = dl.attribute(X)\n",
    "#gs_attr_test = gs.attribute(X, X_train)\n",
    "fa_attr_test = fa.attribute(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_names = [\"kcore_size\", \"seller_pr\", \"buyer_pr\"]\n",
    "\n",
    "# prepare attributions for visualization\n",
    "\n",
    "x_axis_data = np.arange(X.shape[1])\n",
    "x_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n",
    "\n",
    "ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\n",
    "ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)\n",
    "\n",
    "ig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\n",
    "ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(ig_nt_attr_test_sum, ord=1)\n",
    "\n",
    "dl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)\n",
    "dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)\n",
    "\n",
    "fa_attr_test_sum = fa_attr_test.detach().numpy().sum(0)\n",
    "fa_attr_test_norm_sum = fa_attr_test_sum / np.linalg.norm(fa_attr_test_sum, ord=1)\n",
    "\n",
    "lin_weight = nn.lin1.weight[0].detach().numpy()\n",
    "y_axis_lin_weight = lin_weight / np.linalg.norm(lin_weight, ord=1)\n",
    "\n",
    "width = 0.14\n",
    "legends = ['Int Grads', 'Int Grads w/SmoothGrad','DeepLift', 'Feature Ablation', 'Weights']\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_title('Comparing input feature importances across multiple algorithms and learned weights')\n",
    "ax.set_ylabel('Attributions')\n",
    "\n",
    "FONT_SIZE = 16\n",
    "plt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n",
    "\n",
    "ax.bar(x_axis_data, ig_attr_test_norm_sum, width, align='center', alpha=0.8, color='#eb5e7c')\n",
    "ax.bar(x_axis_data + width, ig_nt_attr_test_norm_sum, width, align='center', alpha=0.7, color='#A90000')\n",
    "ax.bar(x_axis_data + 2 * width, dl_attr_test_norm_sum, width, align='center', alpha=0.6, color='#34b8e0')\n",
    "ax.bar(x_axis_data + 4 * width, fa_attr_test_norm_sum, width, align='center', alpha=1.0, color='#49ba81')\n",
    "ax.bar(x_axis_data + 5 * width, y_axis_lin_weight, width, align='center', alpha=1.0, color='grey')\n",
    "ax.autoscale_view()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_xticks(x_axis_data + 0.5)\n",
    "ax.set_xticklabels(x_axis_data_labels)\n",
    "\n",
    "plt.legend(legends, loc=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kdd2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58e1c476a485e41cfbe41589baaac1849e255b67efca409760b0ec34d7f4c191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
