{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyTigerGraph as tg\n",
    "\n",
    "conn = tg.TigerGraphConnection(\"http://3.22.188.182\", graphname=\"KDD_2022_NFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tg_fastRP'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = conn.gds.featurizer()\n",
    "\n",
    "f.installAlgorithm(\"tg_fastRP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"v_type\": [\"Category\", \"NFT_Collection\", \"NFT\"], \n",
    "          \"e_type\": [\"COLLECTION_HAS_NFT\", \"CATEGORY_HAS_NFT\", \"NFT_IN_CATEGORY\", \"NFT_IN_COLLECTION\"], \n",
    "          \"weights\": \"1,2,4\", \n",
    "          \"beta\": -0.1,\n",
    "          \"k\": 3,\n",
    "          \"reduced_dim\": 64, \n",
    "          \"sampling_constant\": 3,\n",
    "          \"random_seed\": 42,\n",
    "          \"print_accum\": False,\n",
    "          \"result_attr\": \"fastrp_embedding\"}\n",
    "\n",
    "f.runAlgorithm(\"tg_fastRP\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExprFunctions=\"\"  # For enterprise users, please use the link you received.\n",
    "ExprUtil=\"\"  # For enterprise users, please use the link you received.\n",
    "#conn.installUDF(ExprFunctions, ExprUtil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.gds.configureKafka(kafka_address=\"kaf.kdd.tigergraphlabs.com:19092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats={\"Transaction\": [\"seller_k_size\", \"buyer_k_size\"], \n",
    "                \"NFT_User\": [\"pagerank\", \"kcore_size\"], \n",
    "                \"NFT\": [\"fastrp_embedding\"], \n",
    "                \"NFT_Collection\": [\"fastrp_embedding\"], \n",
    "                \"Category\": [\"fastrp_embedding\"]},\n",
    "    v_out_labels={\"Transaction\": [\"usd_price\"]},\n",
    "    v_extra_feats={\"Transaction\":  [\"train\"]},\n",
    "    filter_by={\"Transaction\": \"train\"},\n",
    "    shuffle=True,\n",
    "    batch_size=2048,\n",
    "    buffer_size=4,\n",
    "    add_self_loop=True,\n",
    "    reverse_edge=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Transaction', 'NFT', 'NFT_User', 'NFT_Collection', 'Category'], [('Transaction', 'NFT_SOLD_BY', 'NFT_User'), ('Transaction', 'NFT_BOUGHT_BY', 'NFT_User'), ('Transaction', 'FOR_SALE_OF', 'NFT'), ('NFT', 'HAD_TRANSACTION', 'Transaction'), ('NFT', 'NFT_IN_COLLECTION', 'NFT_Collection'), ('NFT', 'NFT_IN_CATEGORY', 'Category'), ('NFT_User', 'USER_SOLD_NFT', 'Transaction'), ('NFT_User', 'USER_SOLD_TO', 'NFT_User'), ('NFT_User', 'USER_BOUGHT_FROM', 'NFT_User'), ('NFT_User', 'USER_BOUGHT_NFT', 'Transaction')])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.metadata())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a normal (homogeneous) GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, out_dim, dropout, hidden_dim, num_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_units = (-1, -1) if i == 0 else hidden_dim * num_heads\n",
    "            out_units = out_dim if i == (num_layers - 1) else hidden_dim\n",
    "            heads = 1 if i == (num_layers - 1) else num_heads\n",
    "            self.layers.append(\n",
    "                GATConv(in_units, out_units, heads=heads, dropout=dropout)\n",
    "            )\n",
    "        self.double()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.float()\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return x\n",
    "    \n",
    "model = GAT(\n",
    "    num_layers=2,\n",
    "    out_dim=1,\n",
    "    dropout=0.8,\n",
    "    hidden_dim=8,\n",
    "    num_heads=4,\n",
    ")\n",
    "\n",
    "# Convert it to a heterogeneous model. See https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.to_hetero_transformer.to_hetero for details.\n",
    "model = to_hetero(model, batch.metadata(), aggr='mul').to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "mae = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/gnn_training'+str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 LOSS: 78.77748285543676 MAE: 79.13949146116086 R2: -0.004093586434539757\n",
      "EPOCH: 1 LOSS: 78.02543941923834 MAE: 78.38746651635485 R2: -0.0038777763290915868\n",
      "EPOCH: 2 LOSS: 78.29817783478056 MAE: 78.65913308409934 R2: -0.0035152458391657\n",
      "EPOCH: 3 LOSS: 76.12090217557524 MAE: 76.48212772647099 R2: -0.004335519889594991\n",
      "EPOCH: 4 LOSS: 77.37672537726861 MAE: 77.73714511261376 R2: -0.003326445372492428\n",
      "EPOCH: 5 LOSS: 77.74104996748152 MAE: 78.10271661141279 R2: -0.0036487811634404546\n",
      "EPOCH: 6 LOSS: 78.37719092397855 MAE: 78.74018209571047 R2: -0.003818260796842915\n",
      "EPOCH: 7 LOSS: 79.44608394166376 MAE: 79.80940368010646 R2: -0.003334035389868012\n",
      "EPOCH: 8 LOSS: 77.56743382232611 MAE: 77.93013261272456 R2: -0.0039340941368103495\n",
      "EPOCH: 9 LOSS: 78.91554735492657 MAE: 79.27778628659708 R2: -0.004264730599281045\n",
      "EPOCH: 10 LOSS: 78.26720706900942 MAE: 78.62839417482651 R2: -0.0038934333382166044\n",
      "EPOCH: 11 LOSS: 77.59721180030512 MAE: 77.95783193299086 R2: -0.004212503220781094\n",
      "EPOCH: 12 LOSS: 77.19809745885696 MAE: 77.55682826708319 R2: -0.004201700819715835\n",
      "EPOCH: 13 LOSS: 76.8093825836268 MAE: 77.1669897641013 R2: -0.0038265110406287737\n",
      "EPOCH: 14 LOSS: 78.6645282760921 MAE: 79.01991182345589 R2: -0.003698274539568641\n",
      "EPOCH: 15 LOSS: 79.07484234611901 MAE: 79.42786461726651 R2: -0.003880476334272409\n",
      "EPOCH: 16 LOSS: 79.99944945863906 MAE: 80.34999900972811 R2: -0.0037313688873770734\n",
      "EPOCH: 17 LOSS: 77.77347617801958 MAE: 78.12106648334368 R2: -0.003951887273110373\n",
      "EPOCH: 18 LOSS: 80.07008415714657 MAE: 80.41635635651146 R2: -0.003899948803678175\n",
      "EPOCH: 19 LOSS: 77.92283991794666 MAE: 78.26881488703378 R2: -0.0038377353116177773\n"
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=5),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./runs/gnn_training/profiler'),\n",
    "        record_shapes=True\n",
    ") as prof:\n",
    "    for i in range(20):\n",
    "        epochLoss = 0\n",
    "        epochMae = 0\n",
    "        epochR2 = 0\n",
    "        j = 0\n",
    "        for batch in train_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x_dict, batch.edge_index_dict)\n",
    "            mask = batch[\"Transaction\"].train\n",
    "            loss = F.smooth_l1_loss(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochLoss += loss.item()\n",
    "            batchR2 = r2_loss(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask]).item()\n",
    "            epochR2 += batchR2\n",
    "            batchMae = mae(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "            epochMae += batchMae.item()\n",
    "            #print(\"Batch:\", j, \"Loss:\", loss.item(), \"MAE:\", batchMae.item())\n",
    "\n",
    "                    # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            loss.item(),\n",
    "                            i * train_loader.num_batches + j)\n",
    "            writer.add_scalar('training mae',\n",
    "                              batchMae.item(),\n",
    "                              i * train_loader.num_batches + j)\n",
    "            writer.add_scalar('training R2',\n",
    "                              batchR2,\n",
    "                              i * train_loader.num_batches + j)\n",
    "\n",
    "            j += 1\n",
    "            prof.step()\n",
    "        print(\"EPOCH:\", i, \"LOSS:\", epochLoss / train_loader.num_batches, \"MAE:\", epochMae / train_loader.num_batches, \"R2:\", epochR2 / train_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and optimizing queries. It might take a minute if this is the first time you use this loader.\n",
      "Query installation finished.\n"
     ]
    }
   ],
   "source": [
    "test_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats={\"Transaction\": [\"seller_k_size\", \"buyer_k_size\"], \n",
    "                \"NFT_User\": [\"pagerank\", \"kcore_size\"], \n",
    "                \"NFT\": [\"fastrp_embedding\"], \n",
    "                \"NFT_Collection\": [\"fastrp_embedding\"], \n",
    "                \"Category\": [\"fastrp_embedding\"]},\n",
    "    v_out_labels={\"Transaction\": [\"usd_price\"]},\n",
    "    v_extra_feats={\"Transaction\":  [\"test\"]},\n",
    "    filter_by={\"Transaction\": \"test\"},\n",
    "    shuffle=False,\n",
    "    batch_size=2048,\n",
    "    add_self_loop=True,\n",
    "    reverse_edge=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 107.91221425399716 MAE: 108.29418413933134 R2: -0.006296718814673345\n"
     ]
    }
   ],
   "source": [
    "totLoss = 0\n",
    "totMAE = 0\n",
    "totR2 = 0\n",
    "for batch in test_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        mask = batch[\"Transaction\"].test\n",
    "        loss = F.smooth_l1_loss(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "    totMAE += mae(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask]).item()\n",
    "    totLoss += loss.item()\n",
    "    totR2 += r2_loss(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask]).item()\n",
    "print(\"LOSS:\", totLoss / test_loader.num_batches, \"MAE:\", totMAE / test_loader.num_batches, \"R2:\", totR2 / test_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TigerGraph Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "58e1c476a485e41cfbe41589baaac1849e255b67efca409760b0ec34d7f4c191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
