{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Graph Neural Network\n",
    "## Notebook 3\n",
    "\n",
    "In this notebook, we will define, train, and test a Graph Neural Network to predict sale prices of NFTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connect to TigerGraph Database\n",
    "\n",
    "The code block below connects to a TigerGraph database. Make sure to change the authentication details in order for you to connect to the instance successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyTigerGraph as tg\n",
    "\n",
    "hostName = \"YOUR_HOSTNAME_HERE\"\n",
    "gsqlSecret = \"YOUR_SECRET_HERE\"\n",
    "graphname= \"KDD_2022_NFT\"\n",
    "\n",
    "conn = tg.TigerGraphConnection(host=hostName, graphname=\"KDD_2022_NFT\", gsqlSecret=gsqlSecret)\n",
    "conn.getToken(gsqlSecret)\n",
    "\n",
    "conn.gds.configureKafka(kafka_address=\"kaf.kdd.tigergraphlabs.com:19092\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Graph Features\n",
    "\n",
    "Some of the vertices don't have features we can use to pass into the Graph Neural Network we are defining later. To fix this, we are using FastRP to generate a feature vector that is a topologically-based embedding of the vertices in the graph we are embedding.\n",
    "\n",
    "We are only running FastRP on Categories, Collections, and NFTs in the graph to prevent data contamination on Transactions. Future improvments could include using image-derived features for NFTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = conn.gds.featurizer()\n",
    "\n",
    "f.installAlgorithm(\"tg_fastRP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"v_type\": [\"Category\", \"NFT_Collection\", \"NFT\"], \n",
    "          \"e_type\": [\"COLLECTION_HAS_NFT\", \"CATEGORY_HAS_NFT\", \"NFT_IN_CATEGORY\", \"NFT_IN_COLLECTION\"], \n",
    "          \"weights\": \"1,2,4\", \n",
    "          \"beta\": -0.1,\n",
    "          \"k\": 3,\n",
    "          \"reduced_dim\": 64, \n",
    "          \"sampling_constant\": 3,\n",
    "          \"random_seed\": 42,\n",
    "          \"print_accum\": False,\n",
    "          \"result_attr\": \"fastrp_embedding\"}\n",
    "\n",
    "f.runAlgorithm(\"tg_fastRP\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Data Loader\n",
    "\n",
    "Here we define a subgraph neighbor loader to train our GNN with. This neighbor loader was introduced in the GraphSAGE paper.\n",
    "\n",
    "By default, 2 hops with 10 neighbors each are used to sample the graph.\n",
    "\n",
    "\n",
    "![GraphSAGE Neighbor Sampler](https://dsgiitr.com/images/blogs/GraphSAGE/GraphSAGE_cover.jpg)\n",
    "\n",
    "**Image Credit: https://dsgiitr.com/blogs/graphsage/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE NEIGHBOR LOADER HERE. SEE code_answers/neighborLoader.py for correct implementation\n",
    "'''\n",
    "train_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats={\"Transaction\": [\"seller_k_size\", \"buyer_k_size\"], \n",
    "                \"NFT_User\": [\"pagerank\", \"kcore_size\"], \n",
    "                \"NFT\": [\"fastrp_embedding\"], \n",
    "                \"NFT_Collection\": [\"fastrp_embedding\"], \n",
    "                \"Category\": [\"fastrp_embedding\"]},\n",
    "    v_out_labels={\"Transaction\": [\"usd_price\"]},\n",
    "    v_extra_feats={\"Transaction\":  [\"train\"]},\n",
    "    filter_by={\"Transaction\": \"train\"},\n",
    "    shuffle=True,\n",
    "    batch_size=2048,\n",
    "    buffer_size=4,\n",
    "    add_self_loop=True,\n",
    "    reverse_edge=True\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.metadata())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Graph Attention Network\n",
    "\n",
    "We define a Graph Attention Network that we will train to perform our regression task. PyTorch Geometric includes a utility to convert homogenous GNN models to work on heterogeneous graphs that we will be utilizing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a normal (homogeneous) GAT model\n",
    "# SEE GAT model definition in code_answers/gat.py for correct implementation\n",
    "'''\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, out_dim, dropout, hidden_dim, num_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_units = (-1, -1) if i == 0 else hidden_dim * num_heads\n",
    "            out_units = out_dim if i == (num_layers - 1) else hidden_dim\n",
    "            heads = 1 if i == (num_layers - 1) else num_heads\n",
    "            self.layers.append(\n",
    "                GATConv(in_units, out_units, heads=heads, dropout=dropout)\n",
    "            )\n",
    "        self.double()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.float()\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return x\n",
    "'''\n",
    "\n",
    "    \n",
    "model = GAT(\n",
    "    num_layers=2,\n",
    "    out_dim=1,\n",
    "    dropout=0.8,\n",
    "    hidden_dim=8,\n",
    "    num_heads=4,\n",
    ")\n",
    "\n",
    "# Convert it to a heterogeneous model. See https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.to_hetero_transformer.to_hetero for details.\n",
    "model = to_hetero(model, batch.metadata(), aggr='mul').to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "mae = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train GNN\n",
    "\n",
    "We will be training the GNN for 20 epochs, and logging the results to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/gnn_training'+str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    epochLoss = 0\n",
    "    epochMae = 0\n",
    "\n",
    "    j = 0\n",
    "    for batch in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        mask = batch[\"Transaction\"].train\n",
    "        loss = F.smooth_l1_loss(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epochLoss += loss.item()\n",
    "        batchMae = mae(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "        epochMae += batchMae.item()\n",
    "        #print(\"Batch:\", j, \"Loss:\", loss.item(), \"MAE:\", batchMae.item())\n",
    "\n",
    "                # ...log the running loss\n",
    "        writer.add_scalar('training loss',\n",
    "                        loss.item(),\n",
    "                        i * train_loader.num_batches + j)\n",
    "        writer.add_scalar('training mae',\n",
    "                          batchMae.item(),\n",
    "                          i * train_loader.num_batches + j)\n",
    "\n",
    "        j += 1\n",
    "    print(\"EPOCH:\", i, \"LOSS:\", epochLoss / train_loader.num_batches, \"MAE:\", epochMae / train_loader.num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test GNN\n",
    "\n",
    "We define the test data loader and then evaluate the GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats={\"Transaction\": [\"seller_k_size\", \"buyer_k_size\"], \n",
    "                \"NFT_User\": [\"pagerank\", \"kcore_size\"], \n",
    "                \"NFT\": [\"fastrp_embedding\"], \n",
    "                \"NFT_Collection\": [\"fastrp_embedding\"], \n",
    "                \"Category\": [\"fastrp_embedding\"]},\n",
    "    v_out_labels={\"Transaction\": [\"usd_price\"]},\n",
    "    v_extra_feats={\"Transaction\":  [\"test\"]},\n",
    "    filter_by={\"Transaction\": \"test\"},\n",
    "    shuffle=False,\n",
    "    batch_size=2048,\n",
    "    add_self_loop=True,\n",
    "    reverse_edge=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totLoss = 0\n",
    "totMAE = 0\n",
    "for batch in test_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        mask = batch[\"Transaction\"].test\n",
    "        loss = F.smooth_l1_loss(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "    totMAE += mae(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask]).item()\n",
    "    totLoss += loss.item()\n",
    "print(\"LOSS:\", totLoss / test_loader.num_batches, \"MAE:\", totMAE / test_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TigerGraph Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "58e1c476a485e41cfbe41589baaac1849e255b67efca409760b0ec34d7f4c191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
