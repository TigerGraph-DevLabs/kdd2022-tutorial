{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Graph Neural Network\n",
    "## Notebook 3\n",
    "\n",
    "In this notebook, we will define, train, and test a Graph Neural Network to predict sale prices of NFTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connect to TigerGraph Database\n",
    "\n",
    "The code block below connects to a TigerGraph database. Make sure to change the authentication details in order for you to connect to the instance successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tigergraph/pyTigerGraph.git@topicDelay\n",
      "  Cloning https://github.com/tigergraph/pyTigerGraph.git (to revision topicDelay) to /tmp/pip-req-build-okiz1m_o\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/tigergraph/pyTigerGraph.git /tmp/pip-req-build-okiz1m_o\n",
      "  Running command git checkout -b topicDelay --track origin/topicDelay\n",
      "  Switched to a new branch 'topicDelay'\n",
      "  branch 'topicDelay' set up to track 'origin/topicDelay'.\n",
      "  Resolved https://github.com/tigergraph/pyTigerGraph.git to commit a23b6dfc951c1de16c53bee117da0ce633f7c7d3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pyTigerGraph\n",
      "  Building wheel for pyTigerGraph (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyTigerGraph: filename=pyTigerGraph-1.0.2-py3-none-any.whl size=113622 sha256=f8ae6edc8348200738ce0613f30f1db9062e1fb5d2cb985f0d20d04e4b3c33ab\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8strfjxu/wheels/51/ba/e4/4f87eb7d55e13c41d3553b2f59d29b3a364289abc341188a9f\n",
      "Successfully built pyTigerGraph\n",
      "Installing collected packages: pyTigerGraph\n",
      "  Attempting uninstall: pyTigerGraph\n",
      "    Found existing installation: pyTigerGraph 1.0.2\n",
      "    Uninstalling pyTigerGraph-1.0.2:\n",
      "      Successfully uninstalled pyTigerGraph-1.0.2\n",
      "Successfully installed pyTigerGraph-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall --no-deps git+https://github.com/tigergraph/pyTigerGraph.git@topicDelay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5jcbhq5c1sdeeq2ekkvljbbq0q0sa1jb', 1662843611, '2022-09-10 21:00:11')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyTigerGraph import TigerGraphConnection\n",
    "\n",
    "conn=TigerGraphConnection(\n",
    "    host=\"YOUR_HOSTNAME_HERE\",\n",
    "    graphname=\"KDD_2022_NFT\",\n",
    "    gsqlSecret=\"YOUR_SECRET_HERE\"\n",
    ")\n",
    "conn.getToken(\"YOUR_SECRET_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "conn.gds.configureKafka(kafka_address=\"kaf.kdd.tigergraphlabs.com:19092\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Graph Features\n",
    "\n",
    "Some of the vertices don't have features we can use to pass into the Graph Neural Network we are defining later. To fix this, we are using FastRP to generate a feature vector that is a topologically-based embedding of the vertices in the graph we are embedding.\n",
    "\n",
    "We are only running FastRP on Categories, Collections, and NFTs in the graph to prevent data contamination on Transactions. Future improvments could include using image-derived features for NFTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tg_fastRP'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = conn.gds.featurizer()\n",
    "\n",
    "f.installAlgorithm(\"tg_fastRP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"v_type\": [\"Category\", \"NFT_Collection\", \"NFT\"], \n",
    "          \"e_type\": [\"COLLECTION_HAS_NFT\", \"CATEGORY_HAS_NFT\", \"NFT_IN_CATEGORY\", \"NFT_IN_COLLECTION\"], \n",
    "          \"weights\": \"1,2,4\", \n",
    "          \"beta\": -0.1,\n",
    "          \"k\": 3,\n",
    "          \"reduced_dim\": 64, \n",
    "          \"sampling_constant\": 3,\n",
    "          \"random_seed\": 42,\n",
    "          \"print_accum\": False,\n",
    "          \"result_attr\": \"fastrp_embedding\"}\n",
    "\n",
    "f.runAlgorithm(\"tg_fastRP\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Data Loader\n",
    "\n",
    "Here we define a subgraph neighbor loader to train our GNN with. This neighbor loader was introduced in the GraphSAGE paper.\n",
    "\n",
    "By default, 2 hops with 10 neighbors each are used to sample the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE NEIGHBOR LOADER HERE. SEE code_answers/neighborLoader.py for correct implementation\n",
    "\n",
    "train_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats={\"Transaction\": [\"seller_k_size\", \"buyer_k_size\"], \n",
    "                \"NFT_User\": [\"pagerank\", \"kcore_size\"], \n",
    "                \"NFT\": [\"fastrp_embedding\"], \n",
    "                \"NFT_Collection\": [\"fastrp_embedding\"], \n",
    "                \"Category\": [\"fastrp_embedding\"]},\n",
    "    v_out_labels={\"Transaction\": [\"usd_price\"]},\n",
    "    v_extra_feats={\"Transaction\":  [\"train\"]},\n",
    "    filter_by={\"Transaction\": \"train\"},\n",
    "    shuffle=True,\n",
    "    batch_size=2048,\n",
    "    buffer_size=4,\n",
    "    add_self_loop=True,\n",
    "    reverse_edge=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Transaction', 'NFT', 'NFT_User', 'NFT_Collection', 'Category'], [('Transaction', 'NFT_SOLD_BY', 'NFT_User'), ('Transaction', 'NFT_BOUGHT_BY', 'NFT_User'), ('Transaction', 'FOR_SALE_OF', 'NFT'), ('NFT_User', 'USER_SOLD_NFT', 'Transaction'), ('NFT_User', 'USER_SOLD_TO', 'NFT_User'), ('NFT_User', 'USER_BOUGHT_FROM', 'NFT_User'), ('NFT_User', 'USER_BOUGHT_NFT', 'Transaction'), ('NFT', 'HAD_TRANSACTION', 'Transaction'), ('NFT', 'NFT_IN_COLLECTION', 'NFT_Collection'), ('NFT', 'NFT_IN_CATEGORY', 'Category')])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.metadata())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Graph Attention Network\n",
    "\n",
    "We define a Graph Attention Network that we will train to perform our regression task. PyTorch Geometric includes a utility to convert homogenous GNN models to work on heterogeneous graphs that we will be utilizing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a normal (homogeneous) GAT model\n",
    "# SEE GAT model definition in code_answers/gat.py for correct implementation\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers, out_dim, dropout, hidden_dim, num_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_units = (-1, -1) if i == 0 else hidden_dim * num_heads\n",
    "            out_units = out_dim if i == (num_layers - 1) else hidden_dim\n",
    "            heads = 1 if i == (num_layers - 1) else num_heads\n",
    "            self.layers.append(\n",
    "                GATConv(in_units, out_units, heads=heads, dropout=dropout)\n",
    "            )\n",
    "        self.double()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.float()\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = GAT(\n",
    "    num_layers=2,\n",
    "    out_dim=1,\n",
    "    dropout=0.8,\n",
    "    hidden_dim=8,\n",
    "    num_heads=4,\n",
    ")\n",
    "\n",
    "# Convert it to a heterogeneous model. See https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.to_hetero_transformer.to_hetero for details.\n",
    "model = to_hetero(model, batch.metadata(), aggr='mul').to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "mae = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train GNN\n",
    "\n",
    "We will be training the GNN for 20 epochs, and logging the results to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/gnn_training'+str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 LOSS: 85.27291761465274 MAE: 85.63408809073924\n",
      "EPOCH: 1 LOSS: 83.98076552629963 MAE: 84.34152881763622\n",
      "EPOCH: 2 LOSS: 86.93331092411626 MAE: 87.2941620931299\n",
      "EPOCH: 3 LOSS: 85.60263742033158 MAE: 85.96256543173915\n",
      "EPOCH: 4 LOSS: 83.39928797380585 MAE: 83.75923085905602\n",
      "EPOCH: 5 LOSS: 86.62987410545702 MAE: 86.99048633737402\n",
      "EPOCH: 6 LOSS: 83.91957104965908 MAE: 84.28048034965917\n",
      "EPOCH: 7 LOSS: 85.02264730016785 MAE: 85.38314597898149\n",
      "EPOCH: 8 LOSS: 84.21897639932214 MAE: 84.57996342240907\n",
      "EPOCH: 9 LOSS: 84.10481188456062 MAE: 84.46582055728906\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    epochLoss = 0\n",
    "    epochMae = 0\n",
    "\n",
    "    j = 0\n",
    "    for batch in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        mask = batch[\"Transaction\"].train\n",
    "        loss = F.smooth_l1_loss(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epochLoss += loss.item()\n",
    "        batchMae = mae(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "        epochMae += batchMae.item()\n",
    "        #print(\"Batch:\", j, \"Loss:\", loss.item(), \"MAE:\", batchMae.item())\n",
    "\n",
    "                # ...log the running loss\n",
    "        writer.add_scalar('training loss',\n",
    "                        loss.item(),\n",
    "                        i * train_loader.num_batches + j)\n",
    "        writer.add_scalar('training mae',\n",
    "                          batchMae.item(),\n",
    "                          i * train_loader.num_batches + j)\n",
    "\n",
    "        j += 1\n",
    "    print(\"EPOCH:\", i, \"LOSS:\", epochLoss / train_loader.num_batches, \"MAE:\", epochMae / train_loader.num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test GNN\n",
    "\n",
    "We define the test data loader and then evaluate the GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
     ]
    }
   ],
   "source": [
    "test_loader = conn.gds.neighborLoader(\n",
    "    v_in_feats={\"Transaction\": [\"seller_k_size\", \"buyer_k_size\"], \n",
    "                \"NFT_User\": [\"pagerank\", \"kcore_size\"], \n",
    "                \"NFT\": [\"fastrp_embedding\"], \n",
    "                \"NFT_Collection\": [\"fastrp_embedding\"], \n",
    "                \"Category\": [\"fastrp_embedding\"]},\n",
    "    v_out_labels={\"Transaction\": [\"usd_price\"]},\n",
    "    v_extra_feats={\"Transaction\":  [\"test\"]},\n",
    "    filter_by={\"Transaction\": \"test\"},\n",
    "    shuffle=False,\n",
    "    batch_size=2048,\n",
    "    add_self_loop=True,\n",
    "    reverse_edge=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 68.7366580001595 MAE: 69.09861176956103\n"
     ]
    }
   ],
   "source": [
    "totLoss = 0\n",
    "totMAE = 0\n",
    "for batch in test_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        mask = batch[\"Transaction\"].test\n",
    "        loss = F.smooth_l1_loss(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask])\n",
    "    totMAE += mae(out[\"Transaction\"][mask].flatten(), batch[\"Transaction\"].y[mask]).item()\n",
    "    totLoss += loss.item()\n",
    "print(\"LOSS:\", totLoss / test_loader.num_batches, \"MAE:\", totMAE / test_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TigerGraph Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "58e1c476a485e41cfbe41589baaac1849e255b67efca409760b0ec34d7f4c191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
